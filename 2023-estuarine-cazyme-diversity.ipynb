{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Set up\n",
    "\n",
    "## 0a. Directories\n",
    "\n",
    "**Main working directory**: `/nesi/nobackup/uoa00348/boey/2023-estuarine-cazyme-diversity/`\n",
    "\n",
    "| Item | Location | Comment |\n",
    "| :--- | :--- | :--- |\n",
    "| MAGs | `/nesi/project/ga02676/Waiwera_project/3.Binning/2.Final_bins` | Original set of final bins curated by Dave, Sze and Carmen. Also contains CheckM results. |\n",
    "| KoFamScan<br>(version 1.3.0) | `/nesi/project/uoa02469/Software/kofam_scan_v1.3.0` | Annie updated the database on 29 Mar 2023. |\n",
    "| Transporter Classification Database<br>(TCDB; 12 Apr 2023) | `/nesi/project/uoa02469/Databases/TCDB_20230412` | Boey updated database on 13 Apr 2023. |\n",
    "| dbCAN<br>(version 11) | `/nesi/project/uoa02469/Databases/dbCAN2_v11` | |\n",
    "| dbCAN-sub<br>(downloaded 11 Aug 2022) | `/nesi/project/uoa02469/Databases/dbCAN-sub_20220811` | |\n",
    "| CAZy<br>(10 Aug 2022) | `/nesi/project/uoa02469/Databases/CAZyDB_20220806` | Compiled by Yin et al. of dbCAN. |\n",
    "| SulfAtlas<br>(version 2.3.1) | `/nesi/project/uoa02469/Databases/SulfAtlas_v2.3.1` | Amino acid sequence database; HMM searchable [here](https://sulfatlas.sb-roscoff.fr/sulfatlashmm/). |\n",
    "| InterProScan (version 5.61-93.0) | `bin/interproscan-5.61-93.0` | Module Java/17 needs to be loaded prior. Also, not all analyses ran, but seems to work fine for Pfam, TIGRFAM, and CDD. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0b. Additional software\n",
    "\n",
    "### InterProScan update\n",
    "\n",
    "NeSI has InterProScan as a module, but their version is 3 years old. Best to have a newer version. \n",
    "\n",
    "These instructions were adapted from [here](https://interproscan-docs.readthedocs.io/en/latest/HowToDownload.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "wget --directory-prefix bin/ https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.61-93.0/interproscan-5.61-93.0-64-bit.tar.gz\n",
    "wget --directory-prefix bin/ https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.61-93.0/interproscan-5.61-93.0-64-bit.tar.gz.md5\n",
    "\n",
    "# Recommended checksum to confirm the download was successful:\n",
    "md5sum -c interproscan-5.61-93.0-64-bit.tar.gz.md5\n",
    "# Must return *interproscan-5.61-93.0-64-bit.tar.gz: OK*\n",
    "# If not - try downloading the file again as it may be a corrupted copy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\[2023-04-13 10:43\\] `wget` is being run on a `screen` named `dl-ips`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\[2023-04-14 14:26\\] InterProScan version 5.61-93.0 installed in `bin`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SignalP-6 GPU\n",
    "\n",
    "Need to ask Dini to help with this. Otherwise, can continue to use CPU version, but only stick to CAZymes. Therefore, filter dbCAN identified CAZymes then run it through SignalP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepTMHMM\n",
    "\n",
    "Potentially need to run this on their [cloud server](https://dtu.biolib.com/DeepTMHMM). This is not ideal, but I don't think I have a choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0c. Additional comments\n",
    "\n",
    "Make sure to copy the notebook and `scripts/` to the backed up project directory at the end of the day.\n",
    "\n",
    "I'll add a script to automate that.\n",
    "\n",
    "Content of `scripts/0-backup_project.sh`:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash -e\n",
    "\n",
    "TARGET=/nesi/project/ga02676/Waiwera_project/boey_work/2023-estuarine-cazyme-diversity/\n",
    "\n",
    "cp -v 2023-estuarine-cazyme-diversity.ipynb $TARGET\n",
    "cp -v -r scripts/ $TARGET\n",
    "\n",
    "```\n",
    "\n",
    "Once there, make sure to commit to github from the project directory.\n",
    "\n",
    "```bash\n",
    "# Stage\n",
    "git add --all\n",
    "\n",
    "# Commit\n",
    "git commit -m \"Today's update\"\n",
    "\n",
    "# Push\n",
    "git push\n",
    "```\n",
    "\n",
    "If a mistake was made:\n",
    "\n",
    "```bash\n",
    "# Reset (change HEAD back to good commit)\n",
    "git reset --hard good_commit\n",
    "\n",
    "# Push\n",
    "git push -f origin <last_good_commit>:<branch>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On `data/` and `results/`\n",
    "\n",
    "`data/` is only selectively backed-up to prevent bloating. `results/` is backed-up but not pushed to the git repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get high quality bins\n",
    "\n",
    "In the MAGs directory, use CheckM stats to filter to copy MAGs with $\\gt$ 70% completeness and $\\lt$ 5% contamination. These bins are copied into `data/0.bins/`.\n",
    "\n",
    "```bash\n",
    "bash scripts/1.get_bins.sh\n",
    "```\n",
    "\n",
    "Contents of `1-get_bins.sh`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash -e\n",
    "\n",
    "# Subset MAGs based on CheckM stats\n",
    "\n",
    "# Directories\n",
    "DIRIN=/nesi/project/ga02676/Waiwera_project/3.Binning/2.Final_bins\n",
    "DIROUT=data/1.bins\n",
    "\n",
    "mkdir -p $DIROUT\n",
    "\n",
    "# Variables\n",
    "COMP=70 # Completeness\n",
    "CONT=5  # Contamination\n",
    "\n",
    "# Subset CheckM results and copy MAGs to data/\n",
    "awk -v comp=\"$COMP\" -v cont=\"$CONT\" -F \"\\t\" '($3 > comp) && ($4 < cont) {print $6}' $DIRIN/checkm_data.txt \\\n",
    "  | xargs -I {} cp $DIRIN/{}.fna $DIROUT/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Predict genes and annotate\n",
    "\n",
    "Predict ORFs from MAGs using `prodigal` in an array.\n",
    "\n",
    "```\n",
    "sbatch scripts/2-ORF_prediction.sl\n",
    "```\n",
    "\n",
    "Contents of `2-ORF_prediction.sl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash -e\n",
    "#SBATCH --job-name=prodigal\n",
    "#SBATCH --account=uoa00348\n",
    "#SBATCH --time=2:00:00\n",
    "#SBATCH --mem=4G\n",
    "#SBATCH --cpus-per-task=2\n",
    "#SBATCH --output=slurm_out/%x.%j.%A_%a.out\n",
    "#SBATCH --error=slurm_err/%x.%j.%A_%a.err\n",
    "#SBATCH --array=0-250\n",
    "#SBATCH --mail-type=BEGIN,END,FAIL\n",
    "#SBATCH --mail-user=jian.sheng.boey@auckland.ac.nz\n",
    "\n",
    "# Reference\n",
    "printf \"%s was called on %s\\n\" \"$(basename $0)\" \"$(date)\"\n",
    "\n",
    "# Modules\n",
    "module purge\n",
    "module load prodigal/2.6.3-GCCcore-7.4.0\n",
    "\n",
    "# Directories\n",
    "DIRIN=data/1.bins\n",
    "DIROUT=data/2.orf_prediction\n",
    "\n",
    "mkdir -p $DIROUT\n",
    "\n",
    "# Variables\n",
    "ARR=($DIRIN/*.fna)\n",
    "INPUT=${ARR[$SLURM_ARRAY_TASK_ID]}\n",
    "NAME=$(basename $INPUT .fna)\n",
    "OUTPUT=${DIROUT}/${NAME}_pred\n",
    "\n",
    "# Run prodigal\n",
    "prodigal \\\n",
    "  -i ${INPUT} \\\n",
    "  -f gff \\\n",
    "  -a ${OUTPUT}.faa \\\n",
    "  -d ${OUTPUT}.fna \\\n",
    "  -o ${OUTPUT}.gff \\\n",
    "  -p single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Job ID:** 34268687"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Combine and clean up predictions\n",
    "\n",
    "Concatenate all ORF predictions then remove metadata from the ORF predictions.\n",
    "\n",
    "```bash\n",
    "bash scripts/2.1-ORF_cleanup.sh\n",
    "```\n",
    "\n",
    "Contents of `2.1-ORF_cleanup.sh`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash -e\n",
    "\n",
    "# Concatenate all ORF predictions and remove metadata from ORF predicted sequences.\n",
    "# Metadata is also consolidated from the GFF files.\n",
    "\n",
    "# Directories\n",
    "DIR=data/2.orf_prediction\n",
    "\n",
    "# Concatenate predictions and remove metadata\n",
    "cat $DIR/*.faa \\\n",
    "  | cut -f 1 -d ' ' \\\n",
    "  > $DIR/allbins_pred.faa\n",
    "\n",
    "# Create another without asterisk for interproscan\n",
    "sed -e 's/\\*//g' $DIR/allbins_pred.faa > $DIR/allbins_pred.noast.faa\n",
    "\n",
    "# Consolidate GFF data\n",
    "printf \"bin\\tnode\\tsource\\ttype\\tstart\\tend\\tgff_score\\tstrand\\tphase\\tseqid\\tpartial\\tstart_type\\trbs_motif\\trbs_spacer\\tgc_cont\\tconf\\tscore\\tcscore\\tsscore\\trscore\\tuscore\\ttscore\\n\" > $DIR/allbins_pred.metadata.tsv\n",
    "\n",
    "for i in $DIR/*.gff; do\n",
    "  bin=$(basename $i .gff)\n",
    "  grep -v '#' $i \\\n",
    "    | sed -e 's/;$//' \\\n",
    "    | sed -e 's/;/\\t/g' \\\n",
    "    | sed -E 's/\\w+=//g' \\\n",
    "    | awk '{FS=\"\\t\"; OFS=\"\\t\"} {split($9, a, \"_\"); $1=$1\"_\"a[2]; print}' \\\n",
    "    | awk -v mag=\"$bin\" '{FS=\"\\t\"; OFS=\"\\t\"} {print mag, $0}' \\\n",
    "    >> $DIR/allbins_pred.metadata.tsv\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ORF prediction (BACKUP REQUIRED)\n",
    "\n",
    "Predict annotations for predicted ORFs against:\n",
    "* KEGG (via KoFamScan)\n",
    "* dbCAN\n",
    "* CAZy\n",
    "* SulfAtlas\n",
    "* TCDB\n",
    "* InterPro\n",
    "\n",
    "Also predict signal peptides (via SignalP-6) and transmembrane proteins (via DeepTMHMM).\n",
    "\n",
    "Keep in mind that with InterProScan, sequences will need to be chunked to around 80,000 sequences per file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 InterProScan\n",
    "\n",
    "Need to split inputs into 80,000 sequences per file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "ml purge\n",
    "ml SeqKit/2.2.0\n",
    "\n",
    "mkdir -p data/tmp\n",
    "\n",
    "seqkit split \\\n",
    "  data/2.orf_prediction/allbins_pred.faa \\\n",
    "  --out-dir data/tmp \\\n",
    "  --by-size 80000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into 10 files.\n",
    "\n",
    "Proceed to InterPro annotations.\n",
    "\n",
    "Contents of `3.1-interproscan.sl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash -e\n",
    "#SBATCH --job-name=ipr5\n",
    "#SBATCH --account=uoa00348\n",
    "#SBATCH --time=2:00:00\n",
    "#SBATCH --mem=24G\n",
    "#SBATCH --cpus-per-task=32\n",
    "#SBATCH --output=slurm_out/%x.%j.%A_%a.out\n",
    "#SBATCH --error=slurm_err/%x.%j.%A_%a.err\n",
    "#SBATCH --array=0-9\n",
    "#SBATCH --mail-type=BEGIN,END,FAIL\n",
    "#SBATCH --mail-user=jian.sheng.boey@auckland.ac.nz\n",
    "\n",
    "# Modules\n",
    "module purge\n",
    "module load Java/17\n",
    "\n",
    "# Directories\n",
    "INDIR=data/tmp\n",
    "OUTDIR=data/3.annotation\n",
    "\n",
    "mkdir -p ${OUTDIR}\n",
    "\n",
    "# Variables\n",
    "interproscan=bin/interproscan-5.61-93.0/interproscan.sh\n",
    "\n",
    "ARR=(${INDIR}/allbins_pred.part*.faa)\n",
    "INFILE=${ARR[$SLURM_ARRAY_TASK_ID]}\n",
    "FILEPART=$(basename ${INFILE} .faa | sed -E 's/.*(part_[0-9]*)/\\1/g')\n",
    "OUTBASE=${OUTDIR}/allbins_pred.interpro.${FILEPART}\n",
    "APPL=Pfam,TIGRFAM,CDD\n",
    "FORMAT=xml,tsv\n",
    "CPU=$(($SLURM_CPUS_PER_TASK - 2))\n",
    "\n",
    "\n",
    "# Run InterProScan\n",
    "$interproscan \\\n",
    "  --applications ${APPL} \\\n",
    "  --cpu ${CPU} \\\n",
    "  --formats ${FORMAT} \\\n",
    "  --input ${INFILE} \\\n",
    "  --output-file-base ${OUTBASE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Job ID:** 34314370"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back up results\n",
    "\n",
    "XML files are compressed then copied to project directory.\n",
    "\n",
    "TSV files are concatenated, compressed, then copied to project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "ml purge\n",
    "ml pigz\n",
    "\n",
    "# Concatenate then compress TSV files\n",
    "cat data/3.annotation/*.tsv | pigz -c -p 4 > results/allbins_pred.interpro.tsv.gz\n",
    "\n",
    "# Archive then compress XML files\n",
    "tar -cvf - data/3.annotation/*interpro.*.xml | pigz -p 4 > results/allbins_pred.interpro.xml.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 dbCAN\n",
    "\n",
    "This will annotate against dbCAN and dbCAN-sub using HMMER3.\n",
    "\n",
    "Contents of `3.2-dbcan.sl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash -e\n",
    "#SBATCH --job-name=dbcan\n",
    "#SBATCH --account=uoa00348\n",
    "#SBATCH --time=06:00:00\n",
    "#SBATCH --mem=8G\n",
    "#SBATCH --cpus-per-task=32\n",
    "#SBATCH --output=slurm_out/%x.%j..out\n",
    "#SBATCH --error=slurm_err/%x.%j..err\n",
    "#SBATCH --mail-type=BEGIN,END,FAIL\n",
    "#SBATCH --mail-user=jian.sheng.boey@auckland.ac.nz\n",
    "\n",
    "# Modules\n",
    "module purge\n",
    "module load HMMER/3.3.2-GCC-11.3.0\n",
    "\n",
    "# Directories\n",
    "INDIR=data/2.orf_prediction\n",
    "OUTDIR=data/3.annotation\n",
    "DBDIR=/nesi/project/uoa02469/Databases\n",
    "\n",
    "# Variables \n",
    "INFILE=${INDIR}/allbins_pred.faa\n",
    "INBASE=$(basename ${INFILE} .faa)\n",
    "\n",
    "## Database for dbCAN2\n",
    "DB1=${DBDIR}/dbCAN2_v11/dbCAN-HMMdb-V11\n",
    "## Output for dbCAN2\n",
    "OUTFILE1=${OUTDIR}/${INBASE}.dbcan.domtbl\n",
    "\n",
    "## Database for dbCAN-sub\n",
    "DB2=${DBDIR}/dbCAN-sub_20220811/dbCAN_sub.hmm\n",
    "## Output for dbCAN-sub\n",
    "OUTFILE2=${OUTDIR}/${INBASE}.dbcan-sub.domtbl\n",
    "\n",
    "## dbCAN2 size \n",
    "SZDB1=$(hmmstat ${DB1} | tail -n 1 | cut -f 1 -d ' ')\n",
    "\n",
    "## dbCAN-sub size\n",
    "SZDB2=$(hmmstat ${DB2} | tail -n 1 | cut -f 1 -d ' ')\n",
    "\n",
    "# Run\n",
    "## dbCAN\n",
    "hmmsearch \\\n",
    "  -Z $SZDB1 \\\n",
    "  --cpu ${SLURM_CPUS_PER_TASK} \\\n",
    "  --domtblout ${OUTFILE1} \\\n",
    "  -o /dev/null \\\n",
    "  $DB1 \\\n",
    "  $INFILE\n",
    "\n",
    "## dbCAN-sub\n",
    "hmmsearch \\\n",
    "  -Z $SZDB2 \\\n",
    "  --cpu ${SLURM_CPUS_PER_TASK} \\\n",
    "  --domtblout ${OUTFILE2} \\\n",
    "  -o /dev/null \\\n",
    "  $DB2 \\\n",
    "  $INFILE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Job ID:** 34375960"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 KEGG (KoFamScan)\n",
    "\n",
    "Remember to funnel the temporary files to `data/tmp/`, then have `KoFamScan` delete the temporary files.\n",
    "\n",
    "Contents of `3.3-kofamscan.sl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash -e\n",
    "#SBATCH --job-name=kofam\n",
    "#SBATCH --account=uoa00348\n",
    "#SBATCH --time=24:00:00\n",
    "#SBATCH --mem=64G\n",
    "#SBATCH --cpus-per-task=32\n",
    "#SBATCH --output=slurm_out/%x.%j.out\n",
    "#SBATCH --error=slurm_err/%x.%j.err\n",
    "#SBATCH --mail-type=BEGIN,END,FAIL\n",
    "#SBATCH --mail-user=jian.sheng.boey@auckland.ac.nz\n",
    "\n",
    "# Modules\n",
    "module purge\n",
    "module load \\\n",
    "  Ruby/3.0.1-GCC-11.3.0 \\\n",
    "  HMMER/3.3.2-GCC-11.3.0 \\\n",
    "  Parallel/20220922\n",
    "\n",
    "# Directories\n",
    "INDIR=data/2.orf_prediction\n",
    "OUTDIR=data/3.annotation\n",
    "SOFTDIR=/nesi/project/uoa02469/Software/kofam_scan_v1.3.0\n",
    "TMPDIR=data/tmp\n",
    "\n",
    "# Variables\n",
    "INFILE=${INDIR}/allbins_pred.faa\n",
    "INBASE=$(basename ${INFILE} .faa)\n",
    "OUTFILE=${OUTDIR}/${INBASE}.kofam.tsv\n",
    "\n",
    "## Software variables\n",
    "kofamscan=${SOFTDIR}/bin/exec_annotation\n",
    "\n",
    "KOLIST=${SOFTDIR}/db/ko_list\n",
    "PROFILE=${SOFTDIR}/db/profiles\n",
    "FORMAT=detail-tsv\n",
    "\n",
    "# Run\n",
    "$kofamscan \\\n",
    "  --format=$FORMAT \\\n",
    "  --profile=$PROFILE \\\n",
    "  --ko-list=$KOLIST \\\n",
    "  --cpu=${SLURM_CPUS_PER_TASK} \\\n",
    "  --tmp-dir=$TMPDIR \\\n",
    "  ${INFILE} \\\n",
    "  > ${OUTFILE}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Job ID:** 34372470"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
